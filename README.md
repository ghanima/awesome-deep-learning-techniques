# awesome-deep-learning-techniques
A curated list of awesome deep learning techniques for deep neural networks training, testing, optimization, regularization etc.
1. Weight Initialization
2. Data/Input Processing
3. Data Augmentation
4. Decreasing/Changing Learning Rate
5. Regularization
6. Optimization/Gradient Descent
7. Normalization
8. Activation Function
9. Segmentation


## Weight Initialization

  ### Xavier Initialization
  ### He Initialization


## Data/Input Processing

  ### Input pipelining
  ### Queues


## Data Augmentation

  ### Random cropping
  ### Random padding
  ### Random horizontal flipping
  ### Random RGB color shifting


## Decreasing/Changing Learning Rate

  ### Learning rate decay
  ### Cyclic learning rate


## Regularization

  ### Weight decay
    a. L2 loss
    b. L1 loss
  ### Dropout


## Optimization/Gradient Descent

  ### Adam Optimizer
  ### SGD with momentum
  ### Nesterov Accelerated Gradient (NAG)
  ### Stochastic Gradient Descent (SGD) 


## Normalization

  ### Batch Normalization
  ### Local Response Normalization (LRN)


## Activation Function

  ### ReLU
  ### Sigmoid
  
  
## Segmentation

  ### FCN
  ### U-Net
  ### DeepLab
  
  

 

