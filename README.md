# awesome-deep-learning-techniques
A curated list of awesome deep learning techniques for deep neural networks training, testing, optimization, regularization etc.

A. Weight Initialization
  1. Xavier Initialization
  2. He Initialization


B. Data/Input Processing
  1. Input pipelining
  2. Queues


C. Data Augmentation


D. Decreasing/Changing Learning Rate
  1. Learning rate decay
  2. Cyclic learning rate


E. Regularization
  1. Weight decay
    a. L2 loss
    b. L1 loss
  2. Dropout


F. Optimization/Gradient Descent
  1. Adam Optimizer
  2. Stochastic Gradient Descent (SGD)
  3. SGD with momentum


G. Normalization
  1. Batch Normalization
  2. Local Response Normalization (LRN)


H. Activation Function
  1. ReLU
  2. Sigmoid
  
 

